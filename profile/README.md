# About
We are the VIVID (**V**oice **I**ntegrated **V**ideo **I**mmersive **D**igital) Avatar Team at ByteDance. We are focusing on audio-visual digital human generation.

# Projects 

## Audio Synthesis
- **MegaTTS3**: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis. [Code](https://github.com/bytedance/MegaTTS3), [Paper](https://arxiv.org/abs/2502.18924)
- Text-to-Speech Synthesis with Chain-of-Thought Style Reasoning.
- **Make-An-Audio 2**: Temporal-Enhanced Text-to-Audio Generation. [Code](https://github.com/bytedance/Make-An-Audio-2), [Paper](https://arxiv.org/abs/2305.18474)

## Video Generation
- **InfinityHuman**: Towards Long-Term Audio-Driven Human Animation. 
- **HumanDiT**: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation. [Demo Page](https://agnjason.github.io/HumanDiT-page/), [Paper](https://arxiv.org/abs/2502.04847) 
- **DiTalker**: Fast and Expressive Audio-Driven Talking Face Generation with Dual Diffusion Transformers.
- **MimicTalk** (_NeurIPS 2024_): Mimicking a personalized and expressive 3D talking face in minutes. [Code](https://github.com/yerfor/MimicTalk), [Paper](https://arxiv.org/abs/2410.06734)
- **Real3D-Portrait** (_ICLR 2024 Spotlight_): One-shot Realistic 3D Talking Portrait Synthesis. [Code](https://github.com/yerfor/Real3DPortrait), [Paper](https://arxiv.org/abs/2401.08503)
- **Ada-TTA**: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis.
